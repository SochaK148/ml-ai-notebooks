{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZ-Hf4OUK5ov"
      },
      "source": [
        "**Movie Review Analysis Using CNNs and RNNs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtD3q-myK55B"
      },
      "source": [
        "*In this machine learning project we preprocess movie review data, and analyze it using both CNNs and RNNs.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07wfbwH3LZcy"
      },
      "source": [
        "# 1. Data Collection and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Large Movie Review Dataset is downloaded using TensorFlow's utility function. This dataset is pivotal for our sentiment analysis project as it contains a substantial number of movie reviews, which are ideal for binary classification tasks.\n",
        "\n",
        "A function is defined to explore the contents of the dataset directory. Understanding the structure of the dataset is crucial for setting up data pipelines and deciding how to preprocess the data.\n"
      ],
      "metadata": {
        "id": "nsCb8cxYZh07"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMOjjus5NWMb",
        "outputId": "ae2d4dd2-ca80-48bc-f73d-e2cb56b8aec9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "84125825/84125825 [==============================] - 4s 0us/step\n",
            "Contents of datasets/aclImdb:\n",
            "- README\n",
            "- imdb.vocab\n",
            "- imdbEr.txt\n",
            "- test\n",
            "- train\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from pathlib import Path\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Download the dataset and extract it\n",
        "root_url = \"https://ai.stanford.edu/~amaas/data/sentiment/\"\n",
        "filename = \"aclImdb_v1.tar.gz\"\n",
        "dataset_url = f\"{root_url}{filename}\"\n",
        "dataset_path = tf.keras.utils.get_file(filename, dataset_url, extract=True, cache_dir=\".\")\n",
        "dataset_dir = Path(dataset_path).with_name(\"aclImdb\")\n",
        "\n",
        "# Define a function to explore the dataset structure\n",
        "def explore_dataset(path):\n",
        "    print(f\"Contents of {path}:\")\n",
        "    for item in sorted(path.iterdir()):\n",
        "        print(f\"- {item.name}\")\n",
        "\n",
        "# Explore the top-level structure\n",
        "explore_dataset(dataset_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load file paths for positive and negative reviews separately. This separation is necessary for labeling the reviews in our binary classification\n",
        "\n",
        "The test set is shuffled and split into validation and test sets. This is a standard practice in machine learning to evaluate models on unseen data.\n",
        "\n",
        "A function `create_dataset` is defined to convert these file paths into a TensorFlow dataset, which is a more efficient format for training models in TensorFlow.\n"
      ],
      "metadata": {
        "id": "SteniMuBaMoX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iraqkFd9NXUq"
      },
      "outputs": [],
      "source": [
        "# Function to get file paths for reviews\n",
        "def get_review_paths(dir_path):\n",
        "    return [str(path) for path in dir_path.glob(\"*.txt\")]\n",
        "\n",
        "# Load file paths for training and testing\n",
        "train_pos_paths = get_review_paths(dataset_dir / \"train\" / \"pos\")\n",
        "train_neg_paths = get_review_paths(dataset_dir / \"train\" / \"neg\")\n",
        "test_pos_paths = get_review_paths(dataset_dir / \"test\" / \"pos\")\n",
        "test_neg_paths = get_review_paths(dataset_dir / \"test\" / \"neg\")\n",
        "\n",
        "# Shuffle and split the test set into validation and test sets\n",
        "np.random.shuffle(test_pos_paths)\n",
        "np.random.shuffle(test_neg_paths)\n",
        "valid_pos_paths = test_pos_paths[:len(test_pos_paths) // 2]\n",
        "valid_neg_paths = test_neg_paths[:len(test_neg_paths) // 2]\n",
        "test_pos_paths = test_pos_paths[len(test_pos_paths) // 2:]\n",
        "test_neg_paths = test_neg_paths[len(test_neg_paths) // 2:]\n",
        "\n",
        "# Function to create a dataset from review paths\n",
        "def create_dataset(pos_paths, neg_paths):\n",
        "    def load_review(path, label):\n",
        "        return tf.io.read_file(path), label\n",
        "\n",
        "    pos_ds = tf.data.Dataset.from_tensor_slices((pos_paths, [1] * len(pos_paths)))\n",
        "    neg_ds = tf.data.Dataset.from_tensor_slices((neg_paths, [0] * len(neg_paths)))\n",
        "\n",
        "    return tf.data.Dataset.concatenate(\n",
        "        pos_ds.map(load_review),\n",
        "        neg_ds.map(load_review)\n",
        "    ).shuffle(buffer_size=10000)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = create_dataset(train_pos_paths, train_neg_paths)\n",
        "valid_dataset = create_dataset(valid_pos_paths, valid_neg_paths)\n",
        "test_dataset = create_dataset(test_pos_paths, test_neg_paths)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The datasets are written to TFRecord files. TFRecord is a simple record-oriented binary format that many TensorFlow applications use for training data. This format is efficient and easy to work with when using large datasets."
      ],
      "metadata": {
        "id": "Qg_rHppGafPV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Fc23ymjqNYvc"
      },
      "outputs": [],
      "source": [
        "from contextlib import ExitStack\n",
        "\n",
        "from tensorflow.train import Example, Features, Feature, BytesList, Int64List\n",
        "\n",
        "def create_text_example(review, label):\n",
        "    return Example(\n",
        "        features=Features(\n",
        "            feature={\n",
        "                \"review\": Feature(bytes_list=BytesList(value=[tf.compat.as_bytes(review)])),\n",
        "                \"label\": Feature(int64_list=Int64List(value=[label])),\n",
        "            }))\n",
        "\n",
        "\n",
        "# Function to write dataset to TFRecord files\n",
        "def write_tfrecords(name, dataset, n_shards=10):\n",
        "    paths = [f\"{name}.tfrecord-{index:05d}-of-{n_shards:05d}\" for index in range(n_shards)]\n",
        "    with ExitStack() as stack:\n",
        "        writers = [stack.enter_context(tf.io.TFRecordWriter(path)) for path in paths]\n",
        "        for index, (review, label) in dataset.enumerate():\n",
        "            shard = index % n_shards\n",
        "            example = create_text_example(review.numpy().decode('utf-8'), label.numpy())\n",
        "            writers[shard].write(example.SerializeToString())\n",
        "    return paths\n",
        "\n",
        "# Write datasets to TFRecord files\n",
        "train_filepaths = write_tfrecords(\"imdb_train\", train_dataset)\n",
        "valid_filepaths = write_tfrecords(\"imdb_valid\", valid_dataset)\n",
        "test_filepaths = write_tfrecords(\"imdb_test\", test_dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A function `preprocess_text` is defined to parse the TFRecord files. Parsing the data is necessary to convert it into a format that our model can be trained on.\n"
      ],
      "metadata": {
        "id": "4RbqG_mkarpx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "v9kcT0A7rDv8"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(tfrecord):\n",
        "    feature_descriptions = {\n",
        "        \"review\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
        "        \"label\": tf.io.FixedLenFeature([], tf.int64, default_value=-1),\n",
        "    }\n",
        "    parsed_example = tf.io.parse_single_example(tfrecord, feature_descriptions)\n",
        "    review = parsed_example[\"review\"]\n",
        "    label = parsed_example[\"label\"]\n",
        "    return review, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TV0TGn4N5ImV",
        "outputId": "507972c8-ade5-41a1-ea1c-19a14c0c8c7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review: Twenty five years ago, I showed this film in some children's classes in Entomology and can still remember the excitement of the kids; they were spellbound! It is not just about the termites who have built and live in the \"Castles of Clay,\" but also about the other animals who use the mounds. There is a fantastic scene in which a cobra fights a monitor lizard while a colony of mongooses watch. It is a not only good for entomology classes, but also for teaching about ecology since there is so much about the interactions between the termites and other organisms and the whole ecology of all of the organisms that live in and around the mounds. <br /><br />I wish it was available on DVD, so that I could watch it again and show others.\n",
            "Label: 1\n"
          ]
        }
      ],
      "source": [
        "# Test with a single TFRecord\n",
        "for raw_record in tf.data.TFRecordDataset(train_filepaths).take(1):\n",
        "    review, label = preprocess_text(raw_record)\n",
        "    print(\"Review:\", review.numpy().decode('utf-8'))\n",
        "    print(\"Label:\", label.numpy())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text data is tokenized and vectorized using TensorFlow's `TextVectorization` layer. This process converts text into numerical data that a neural network can process."
      ],
      "metadata": {
        "id": "eRBfFRALa3bv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WJAsLpx7oJOB"
      },
      "outputs": [],
      "source": [
        "# Define the size of the vocabulary and maximum sequence length\n",
        "vocab_size = 10000\n",
        "max_length = 250\n",
        "\n",
        "# Extracting only the review text for vectorization adaptation\n",
        "# Here, train_dataset is a dataset of (review, label) pairs\n",
        "train_reviews = train_dataset.map(lambda review, label: review)\n",
        "\n",
        "# Create and adapt the TextVectorization layer\n",
        "text_vectorization = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=max_length)\n",
        "text_vectorization.adapt(train_reviews)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The datasets are prepared with batching and prefetching, which are key techniques for efficient data loading."
      ],
      "metadata": {
        "id": "ACiuWNfCa7dU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7MxvRHh9NZ8w"
      },
      "outputs": [],
      "source": [
        "def imdb_dataset(filepaths, shuffle_buffer_size=None, batch_size=32, prefetch_buffer=tf.data.AUTOTUNE):\n",
        "    dataset = tf.data.TFRecordDataset(filepaths, num_parallel_reads=tf.data.AUTOTUNE)\n",
        "    if shuffle_buffer_size:\n",
        "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
        "    dataset = dataset.map(preprocess_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    # Apply text_vectorization to the entire batch\n",
        "    dataset = dataset.map(lambda reviews, labels: (text_vectorization(reviews), labels), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    return dataset.prefetch(prefetch_buffer)\n",
        "\n",
        "# Create the datasets\n",
        "train_set = imdb_dataset(train_filepaths, shuffle_buffer_size=25000, batch_size=32)\n",
        "valid_set = imdb_dataset(valid_filepaths, batch_size=32)\n",
        "test_set = imdb_dataset(test_filepaths, batch_size=32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzoaLCKsM7lo"
      },
      "source": [
        "# 2. Text Analysis Using CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A CNN model is constructed with an Embedding layer followed by Convolutional and Pooling layers. This architecture is typically used for image data but is being adapted here for text data, showcasing the versatility of CNNs.\n"
      ],
      "metadata": {
        "id": "vNxiwv_5bn6F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BbULhngG58FK"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 16\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.Conv1D(128, 5, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling1D(pool_size=4),\n",
        "    tf.keras.layers.Conv1D(64, 5, activation='relu'),\n",
        "    tf.keras.layers.GlobalMaxPooling1D(),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is trained on the movie reviews dataset. The training process involves adjusting the weights of the network to minimize the loss function, which in this case is binary cross-entropy, suitable for binary classification tasks."
      ],
      "metadata": {
        "id": "ViCgdovKbryy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuG9oUmT5-Cv",
        "outputId": "c632b8ad-a981-4975-c290-2b6be95a5619"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "782/782 [==============================] - 38s 47ms/step - loss: 0.4353 - accuracy: 0.7762 - val_loss: 0.3327 - val_accuracy: 0.8513\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 36s 45ms/step - loss: 0.2393 - accuracy: 0.9044 - val_loss: 0.4369 - val_accuracy: 0.8154\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 37s 48ms/step - loss: 0.1562 - accuracy: 0.9415 - val_loss: 0.3751 - val_accuracy: 0.8535\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 37s 48ms/step - loss: 0.0881 - accuracy: 0.9705 - val_loss: 0.4830 - val_accuracy: 0.8469\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 37s 47ms/step - loss: 0.0441 - accuracy: 0.9859 - val_loss: 0.6409 - val_accuracy: 0.8409\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 35s 45ms/step - loss: 0.0229 - accuracy: 0.9936 - val_loss: 0.8032 - val_accuracy: 0.8404\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 37s 48ms/step - loss: 0.0151 - accuracy: 0.9950 - val_loss: 0.9466 - val_accuracy: 0.8390\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 36s 45ms/step - loss: 0.0127 - accuracy: 0.9961 - val_loss: 1.0971 - val_accuracy: 0.8338\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 36s 45ms/step - loss: 0.0133 - accuracy: 0.9957 - val_loss: 1.1206 - val_accuracy: 0.8372\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 36s 46ms/step - loss: 0.0136 - accuracy: 0.9954 - val_loss: 1.1554 - val_accuracy: 0.8398\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "history = model.fit(train_set, epochs=10, validation_data=valid_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model's performance is evaluated on the validation set during training and on the test set after training. The results are promising, with high accuracy, indicating that the model has learned to distinguish between positive and negative reviews effectively."
      ],
      "metadata": {
        "id": "-f8f1qorbupn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(test_set)\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32-LI9oaNPo5",
        "outputId": "93a6a4de-2eef-4e20-ca79-99743ea1f616"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "391/391 [==============================] - 6s 15ms/step - loss: 1.1342 - accuracy: 0.8395\n",
            "Loss:  1.1342061758041382\n",
            "Accuracy:  0.8395199775695801\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JzOETx0NAxi"
      },
      "source": [
        "# 3. Text Analysis Using RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A separate RNN model is constructed with an Embedding layer followed by LSTM layers. RNNs, and particularly LSTMs, are well-suited for sequence data like text, as they can capture temporal dependencies."
      ],
      "metadata": {
        "id": "PeeKofAGb-pS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_rnn = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(20000 + 1, 64),\n",
        "    tf.keras.layers.LSTM(64),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_rnn.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "MV2H7W9-NBTc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The RNN model is also trained and evaluated on the same dataset. The performance metrics are closely observed to compare with the CNN model's performance."
      ],
      "metadata": {
        "id": "AvL31VODcBQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model_rnn.fit(train_set, epochs=10, validation_data=valid_set)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Vst-cNONFJV",
        "outputId": "9ff98173-8a55-44b2-afc1-771f1b14c282"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "782/782 [==============================] - 153s 190ms/step - loss: 0.6802 - accuracy: 0.5552 - val_loss: 0.6880 - val_accuracy: 0.5301\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 153s 196ms/step - loss: 0.6698 - accuracy: 0.5755 - val_loss: 0.6775 - val_accuracy: 0.5528\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 152s 194ms/step - loss: 0.6213 - accuracy: 0.6477 - val_loss: 0.6728 - val_accuracy: 0.5180\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 144s 184ms/step - loss: 0.6194 - accuracy: 0.6302 - val_loss: 0.6338 - val_accuracy: 0.6482\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 146s 187ms/step - loss: 0.5990 - accuracy: 0.6524 - val_loss: 0.6547 - val_accuracy: 0.5813\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 148s 189ms/step - loss: 0.6315 - accuracy: 0.6152 - val_loss: 0.6780 - val_accuracy: 0.5638\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 146s 187ms/step - loss: 0.5851 - accuracy: 0.6647 - val_loss: 0.5935 - val_accuracy: 0.7203\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 154s 197ms/step - loss: 0.5396 - accuracy: 0.7066 - val_loss: 0.6673 - val_accuracy: 0.5971\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 151s 193ms/step - loss: 0.5527 - accuracy: 0.6992 - val_loss: 0.6730 - val_accuracy: 0.5643\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 148s 189ms/step - loss: 0.4942 - accuracy: 0.7712 - val_loss: 0.5183 - val_accuracy: 0.7838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy of the RNN model is slightly lower than the CNN. This difference could be a point of analysis to understand how each model type processes text data differently."
      ],
      "metadata": {
        "id": "y6ahp9aPcF7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy = model_rnn.evaluate(test_set)\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXVcdYZJNcRI",
        "outputId": "149d5fc0-6b05-4831-95de-eca3b5e427b5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "391/391 [==============================] - 16s 41ms/step - loss: 0.5250 - accuracy: 0.7825\n",
            "Loss:  0.5249744057655334\n",
            "Accuracy:  0.782480001449585\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ge3R_aqKNDBp"
      },
      "source": [
        "# 4. Interpretation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CNN model shows higher accuracy compared to the RNN model. This could lead to a discussion on why CNNs might be more effective for this particular dataset, possibly due to their ability to capture local dependencies in text.\n",
        "\n",
        "The results also open up discussions on the trade-offs between different model architectures and how they might be more or less suitable depending on the nature of the dataset and the task at hand.\n",
        "\n",
        "Further experiments could include hyperparameter tuning, experimenting with different architectures, or using pre-trained models for transfer learning to potentially improve the results.\n"
      ],
      "metadata": {
        "id": "cc0q3yxPcR3h"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}